{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53c2d00d",
   "metadata": {},
   "source": [
    "# Tornado Classification using Quantum Machine Learning \n",
    "\n",
    "## Objective\n",
    " Our goal is to classify tornadoes based off strength. There are two overlapping objectives. (I) Classify the tornadoes as weak or strong, and (II) Classify tornadoes into specific categories. Weak is defined as anything under an EF of 2, and strong is a tornado of EF 2 or greator. We evaluate the model based off area under the curve (AUC) for the graph of false positive vs true positive. By convention/definition, AUC is between 0.5 (worst case) and 1.0 (best case). We also report a confusion matrix to show how the models perform for each datapoint.\n",
    "\n",
    "## Features and  Definitions\n",
    " \n",
    " * **Cape** - Convective Available Potential Energy: measure of\n",
    "atmospheric instability and thunderstorm potential (J/kg)\n",
    "\n",
    "* **Cin** - Convective Inhibition: atmospheric energy barrier that\n",
    "suppresses convective development (J/kg)\n",
    "\n",
    "* **dewpoint_2m** - 2-meter dewpoint temperature: moisture\n",
    "content indicator at surface level (°C)\n",
    "\n",
    "* **temp_2m** - 2-meter air temperature: surface-level atmospheric\n",
    "temperature measurement (°C)\n",
    "\n",
    "* **tcwv** - Total Column Water Vapor: integrated atmospheric\n",
    "moisture content from surface to top of atmosphere (kg/m²)\n",
    "surface_pressure - Surface atmospheric pressure: weight of air\n",
    "column at ground level (hPa or mb)\n",
    "\n",
    "* **shear_0_1km** - Wind shear 0-1km: difference in wind\n",
    "speed/direction between surface and 1km altitude (m/s)\n",
    "\n",
    "* **shear_0_3km** - Wind shear 0-3km: difference in wind\n",
    "speed/direction between surface and 3km altitude (m/s)\n",
    "\n",
    "* **ef_class** - Enhanced Fujita Scale class: tornado intensity rating from EF0 (weakest) to EF3+ (strongest)\n",
    "\n",
    "* **ef_binary** - Binary tornado classification: simplified weak (EF0-1) versus strong (EF2+) tornado categories\n",
    "\n",
    "## GPU Highlights:\n",
    "Our task and solution would be difficult to solve, and certainly not possible to solve in a reaonable amount of time without GPUs. We highlight the three main areas where GPUs are crucial to our plan:\n",
    "\n",
    "* **AI Model Training** Our problem is an ML classification challenge. Fundamentally, AI benefits from GPUs because of the heavy linear algebra involved in ML. \n",
    "\n",
    "* **Random Shadows** This method requires calculating the expectation value of several observables for each datapoint. Each datum is represented by a single quantum circuit, and each circuit runs multiple observables. In the code below, we run 1000 quantum circuits with 32 observables each. This is considered small. GPU parallelization is imperative to scale this method efficiently.\n",
    "  \n",
    "* **Quantum Kernel Estimation** Our quantum pipeline requires QKE, which requires requires a quadratic number of circuit executions compared to the original input in order to estimate the kernel matrix. This method is often bottlenecked because of the time required to simulate quantum hardware. This is difficult with GPUs; it is not a hyperbole to say it is pragmatically impossible on CPUs.\n",
    "  \n",
    "\n",
    "## Challenges\n",
    "We faced a several challenges when approaching this project. The first is the unbalanced training set. The cell below highlights this imbalance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "882dacc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strong Tornadoes: 107, Weak Tornadoes: 893, Total: 1000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "threshold = 1\n",
    "count_strong = 0\n",
    "count_total = 0\n",
    "\n",
    "for file in [\n",
    "    '../2025-Quantathon-Tornado-Q-training_data-640-examples.xlsx',\n",
    "    '../2025-Quantum-Tornado-Q-test_data-200-examples.xlsx',\n",
    "    '../2025-Quantum-Tornado-validation_data-160-examples.xlsx'\n",
    "]:\n",
    "    df = pd.read_excel(file)\n",
    "    count_strong += (df['ef_class'] > threshold).sum()\n",
    "    count_total  += len(df)  # total number of rows, not sum of ef_class\n",
    "\n",
    "print(f\"Strong Tornadoes: {count_strong}, Weak Tornadoes: {count_total - count_strong}, Total: {count_total}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede04338",
   "metadata": {},
   "source": [
    "As seen above, less than 11% of the dataset contains strong tornadoes. To combat this, we used Smote to create synthetic datapoints via k-nearest neighbors, creating a balanced dataset. Another approach we tried was to use Quantum Kernel Estimation (QKE) on the 107 datapoints so that if a binary classification model identified a tornado as strong, QKE could specify 2 or 3. This method, shown in the cell below, is resource intensive, **requiring GPUs because we need a quadratic number of quantum circuit executions for QKE.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7662fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from Norway for QKE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a7df7a",
   "metadata": {},
   "source": [
    "As many with machine learning experience can quickly notice, 8 (removing EF class and EF binary) features and 1000 datapoints is an extremely small dataset. We use a method known as \"random shadows\" to create more features. A simple explanation of this is as follows: we take each feature and project it into a higher-dimensional space and then look at the \"shadow\" cast from different angles. These shadows are expectation values for different bases, which can act as features themselves. This allows us to enhance our feature space from 8 features to 40 features, giving the model a richer set of representations to learn from. It is important to note that we can scale significantly further than 40 features. This arbitrary number was chosen to fit with the constraints of available resources. Even more features would increase the need for GPU resources.\n",
    "\n",
    "We identify two imperative use cases for GPUs here:\n",
    "\n",
    "* **Model training efficiency**: Increasing the number of features expands the input space and makes the dataset more complex, amplifying the computational demands of training. This naturally increases the benefits of GPU acceleration for tasks like batch processing, larger batch sizes, and deeper epochs during optimization.\n",
    "\n",
    "* **Post-processing of the random shadows**: Once the quantum device provides measurement outcomes, the heavy lifting becomes entirely classical. Computing expectation values, reconstructing observables, and generating shadow-based features all involve large, independent linear-algebraic operations—perfectly suited for GPUs. Each shadow can be processed in parallel, and thousands of random bases can be handled simultaneously. This allows GPUs to accelerate the transformation from raw quantum measurements to classical feature vectors, turning quantum-generated data into usable machine-learning inputs in real time.\n",
    "\n",
    "To illustrate the computational intensity of the post-processing stage, consider a system with $n=20$ qubits, $m=10^5$ measurement shots, and \n",
    "$M=5000$ (potential features). The total number of independent operations required to estimate all observables is approximately: $m×M=10^5×5000=5×10^8$\n",
    "\n",
    "These are small, independent computations — precisely the kind of workload that benefits from GPU parallelism. A modern GPU such as the NVIDIA A100, capable of roughly 20 TFLOP/s, can perform on the order of $10^{12}$ floating-point operations per second. This means that processing $5×10^8$ operations can be completed in milliseconds. To put this into persepctive, generating 32 features for the entire 1000 point dataset took roughly 10-12 minutes on a cpu.\n",
    "\n",
    "In practice, this enables near real-time transformation of quantum measurement data into classical feature vectors. As the number of features or qubits increases, GPU acceleration becomes not only advantageous but essential for maintaining feasible post-processing times.\n",
    "\n",
    "For small workloads (such as ours), the fixed overhead of GPU data transfer and kernel launch time dominates the total runtime, leading to similar performance as CPU computation. However, as the dataset or number of observables scales, the arithmetic intensity (ratio of computation to memory transfer) increases, allowing GPUs to fully utilize their parallel throughput.\n",
    "\n",
    "The cell below gives a short demonstration of creating random shadows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0d8ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "We have 1000 8 qubit circuits. We will need to run each 32 times to generate 32 new features.\n",
      "Random Shadow Generation: 32 observables, 1000 8 qubit circuits\n",
      "GPU Time: 0.00 seconds\n"
     ]
    }
   ],
   "source": [
    "from utils import shadows as sh\n",
    "from utils.circuits import build_circuit\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def normalize_new_features(unnormalized_features):\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "    normalized_features = scaler.fit_transform(unnormalized_features)\n",
    "    return normalized_features\n",
    "\n",
    "\n",
    "# Define the observables we will use for random shadows.\n",
    "ring_paulis = 'XY'\n",
    "\n",
    "# Import a ready to use data set\n",
    "df_train = pd.read_csv('../Data/X_train_scaled.csv')\n",
    "df_val = pd.read_csv('../Data/X_val_scaled.csv')\n",
    "df_test = pd.read_csv('../Data/X_test_scaled.csv')\n",
    "\n",
    "# We combine the entire dataset into one dataframe\n",
    "df = pd.concat([df_train, df_val, df_test], ignore_index= True)\n",
    "\n",
    "# We have an irrelevant feature at the end, which we drop\n",
    "df.drop(df.columns[-1], axis = 1, inplace = True)\n",
    "\n",
    "# Convert the data to a numpy array\n",
    "data = df.to_numpy()\n",
    "\n",
    "# n defines the number of qubits. This is 1 qubit per feature\n",
    "n = len(data[0])\n",
    "\n",
    "# Each data point gets a circuit\n",
    "circuits = [build_circuit(x, entanglement='full', gate='cx', num_layers = 1) for x in data]\n",
    "\n",
    "\n",
    "print(f\"We have {len(circuits)} {n} qubit circuits. We will need to run each 32 times to generate 32 new features.\")\n",
    "\n",
    "# This creates the 32 observables to measure the expectation value of.\n",
    "paulis = sh.paulis_singles_xyz(n) + sh.paulis_ring_pairs(n, (ring_paulis[0], ring_paulis[1]))\n",
    "\n",
    "# This is just some configuration for the backend.\n",
    "cfg = sh.ShadowConfig(T=200, shots = 1000, seed = 123)\n",
    "\n",
    "# This is where the random shadow measurements actually happen\n",
    "start_gpu_clock = time.time()\n",
    "new_features = sh.build_feature_matrix_from_circuits(circuits, paulis, cfg, device = 'GPU')\n",
    "end_gpu_clock = time.time()\n",
    "\n",
    "print(f\"Random Shadow Generation: {len(paulis)} observables, {len(circuits)} {n} qubit circuits\")\n",
    "print(f\"GPU Time: {(end_gpu_clock - start_gpu_clock):.2f} seconds\")\n",
    "\n",
    "# This is the same thing as above but using CPU to show time difference\n",
    "start_cpu_clock = time.time()\n",
    "new_features = sh.build_feature_matrix_from_circuits(circuits, paulis, cfg, device = 'CPU')\n",
    "end_cpu_clock = time.time()\n",
    "\n",
    "print(f\"CPU Time: {(end_cpu_clock - start_cpu_clock):.2f} seconds\")\n",
    "\n",
    "# Normalize the new features\n",
    "normalized_features = normalize_new_features(new_features)\n",
    "\n",
    "df_random_shadows = pd.DataFrame(normalized_features)\n",
    "df_random_shadows.to_csv(f\"./random_shadow_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d98335-feda-4810-9be7-3533e2fe9494",
   "metadata": {},
   "source": [
    "# Trained Models\n",
    "\n",
    "## Classical Binary Model\n",
    "\n",
    "We have two classical models which serve as a baseline for our hybrid and quantum models. The first of the two is the binary classification model. It will classify tornadoes as \"weak\" or strong\" where \"weak\" is defined as EF 0 and EF 1, and \"strong\" is categorized as EF 2 and EF 3. We showcase this model in the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163bab3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.model_definitions import BinaryDNN_classical\n",
    "import torch\n",
    "from utils.imports import *\n",
    "from utils.Preprocessing import *\n",
    "from utils.Helper import *\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DNN = BinaryDNN_classical().to(device)\n",
    "\n",
    "load_path = \"checkpoints/Binary_DNN_FULLENT_AUC.6993.1011_0915.pt\"\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(load_path)\n",
    "\n",
    "# Restore model weights\n",
    "DNN.load_state_dict(checkpoint[\"DNN_state_dict\"])\n",
    "    \n",
    "print(f\"Loaded model from {load_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bce587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils.Preprocessing import Preprocess, ClassificationDataset, DataLoader\n",
    "TRAIN_FILE = '../Data/2025-Quantathon-Tornado-Q-training_data-640-examples.xlsx'\n",
    "TEST_FILE = '../Data/2025-Quantum-Tornado-Q-test_data-200-examples.xlsx'\n",
    "VALIDATION_FILE = '../Data/2025-Quantum-Tornado-validation_data-160-examples.xlsx'\n",
    "\n",
    "# Load training data\n",
    "df_train = pd.read_excel(TRAIN_FILE)\n",
    "# Load test data\n",
    "df_test = pd.read_excel(TEST_FILE)\n",
    "# Load validation data\n",
    "df_val = pd.read_excel(VALIDATION_FILE)\n",
    "\n",
    "# Define variable and dataset\n",
    "batch_size = 64\n",
    "lr = 1e-2\n",
    "\n",
    "X_train, y_train, X_test, y_test, X_val, y_val = Preprocess(df_train, df_test, df_val, balance = 'smote', classes = 'binary')\n",
    "\n",
    "print(y_test[:])\n",
    "print(y_train[:])\n",
    "print(y_val[:])\n",
    "\n",
    "train_data = ClassificationDataset(X_train, y_train)\n",
    "validation_data = ClassificationDataset(X_val, y_val)\n",
    "test_data = ClassificationDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(validation_data, batch_size=64, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False, drop_last=True)\n",
    "\n",
    "X_batch, y_batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81347861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set model to evaluation mode ---\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.Helper import plot_confusion_matrix\n",
    "DNN.eval()\n",
    "\n",
    "all_targets = []\n",
    "all_preds = []\n",
    "all_outputs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for features, target in test_loader:\n",
    "        features = features.to(device)\n",
    "        target = target.to(device).float().unsqueeze(-1)\n",
    "\n",
    "        outputs = DNN(features)\n",
    "        preds = (outputs > 0.5).float()\n",
    "\n",
    "        all_targets.append(target.cpu())\n",
    "        all_preds.append(preds.cpu())\n",
    "        all_outputs.append(outputs.cpu())\n",
    "\n",
    "# --- Concatenate all batches ---\n",
    "all_targets = torch.cat(all_targets).squeeze().long().numpy()  # integers 0/1\n",
    "all_preds = torch.cat(all_preds).squeeze().long().numpy()\n",
    "all_outputs = torch.cat(all_outputs).squeeze().numpy()             # floats in [0,1]\n",
    "\n",
    "# --- Sanity check ---\n",
    "print(all_targets.shape, all_preds.shape, all_outputs.shape)\n",
    "print(np.unique(all_targets))  # should be [0,1]\n",
    "\n",
    "# --- Compute Metrics ---\n",
    "cm = confusion_matrix(all_targets, all_preds)\n",
    "auc_score = roc_auc_score(all_targets, all_outputs)  # should work now\n",
    "f1 = f1_score(all_targets, all_preds)\n",
    "acc = accuracy_score(all_targets, all_preds)\n",
    "\n",
    "# Critical Success Index (CSI)\n",
    "tp = cm[1,1]\n",
    "fn = cm[1,0]\n",
    "fp = cm[0,1]\n",
    "csi = tp / (tp + fn + fp)\n",
    "\n",
    "print(f\"AUC: {auc_score:.4f}, F1: {f1:.4f}, Accuracy: {acc:.4f}, CSI: {csi:.4f}\")\n",
    "\n",
    "# --- Plot Confusion Matrix ---\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# --- Plot ROC Curve ---\n",
    "fpr, tpr, thresholds = roc_curve(all_targets, all_outputs)\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(fpr, tpr, label=f'AUC = {auc_score:.4f}', color='blue')\n",
    "plt.plot([0,1], [0,1], linestyle='--', color='gray')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ae938e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "df_train = pd.read_excel(TRAIN_FILE)\n",
    "# Load test data\n",
    "df_test = pd.read_excel(TEST_FILE)\n",
    "# Load validation data\n",
    "df_val = pd.read_excel(VALIDATION_FILE)\n",
    "\n",
    "\n",
    "\n",
    "X_train, y_train, X_test, y_test, X_val, y_val = Preprocess(df_train, df_test, df_val, balance = 'smote', classes = 'multiclass')\n",
    "\n",
    "train_data = ClassificationDataset(X_train, y_train)\n",
    "validation_data = ClassificationDataset(X_val, y_val)\n",
    "test_data = ClassificationDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(validation_data, batch_size=64, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False, drop_last=True)\n",
    "\n",
    "X_batch, y_batch = next(iter(train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cf5e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.model_definitions import MultiClassDNN_classical\n",
    "\n",
    "DNN = MultiClassDNN_classical().to(device)\n",
    "\n",
    "load_path = \"checkpoints/Multiclass_DNN_3Layer_AUC.8.1011_1012.pt\"\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(load_path)\n",
    "\n",
    "# Restore model weights\n",
    "DNN.load_state_dict(checkpoint[\"DNN_state_dict\"])\n",
    "\n",
    "DNN.eval()\n",
    "all_targets, all_preds, all_probs = [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for features, target in test_loader:\n",
    "        features = features.to(device)\n",
    "        target = target.to(device).long()\n",
    "        outputs = DNN(features)\n",
    "        \n",
    "        if outputs.shape[1] == 1:\n",
    "            # Binary case\n",
    "            probs = torch.sigmoid(outputs).squeeze(-1)\n",
    "            preds = (probs > 0.5).long()\n",
    "        else:\n",
    "            # Multiclass case\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "        all_targets.append(target.cpu())\n",
    "        all_preds.append(preds.cpu())\n",
    "        all_probs.append(probs.cpu())\n",
    "\n",
    "# Concatenate\n",
    "all_targets = torch.cat(all_targets).numpy()\n",
    "all_preds = torch.cat(all_preds).numpy()\n",
    "all_probs = torch.cat(all_probs).numpy()\n",
    "\n",
    "# Metrics\n",
    "cm = confusion_matrix(all_targets, all_preds)\n",
    "acc = accuracy_score(all_targets, all_preds)\n",
    "if outputs.shape[1] == 1:\n",
    "    f1 = f1_score(all_targets, all_preds)\n",
    "    auc_score = roc_auc_score(all_targets, all_probs)\n",
    "    csi = cm[1,1] / (cm[1,1] + cm[1,0] + cm[0,1])\n",
    "else:\n",
    "    num_classes = outputs.shape[1]\n",
    "    f1 = f1_score(all_targets, all_preds, average='macro')\n",
    "    auc_score = roc_auc_score(np.eye(num_classes)[all_targets], all_probs, multi_class='ovr')\n",
    "    csi = []\n",
    "    for i in range(num_classes):\n",
    "        tp = cm[i,i]\n",
    "        fn = cm[i,:].sum() - tp\n",
    "        fp = cm[:,i].sum() - tp\n",
    "        csi.append(tp / (tp + fn + fp) if (tp+fn+fp)>0 else 0)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"AUC: {auc_score:.4f}, F1: {f1:.4f}, Accuracy: {acc:.4f}\")\n",
    "if outputs.shape[1] > 1:\n",
    "    for i, c in enumerate(csi):\n",
    "        print(f\"Class {i} CSI: {c:.4f}\")\n",
    "else:\n",
    "    print(f\"CSI: {csi:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# ROC curve\n",
    "plt.figure(figsize=(7,6))\n",
    "if outputs.shape[1] == 1:\n",
    "    fpr, tpr, _ = roc_curve(all_targets, all_probs)\n",
    "    plt.plot(fpr, tpr, label=f'AUC={auc_score:.4f}')\n",
    "else:\n",
    "    for i in range(num_classes):\n",
    "        fpr, tpr, _ = roc_curve((all_targets==i).astype(int), all_probs[:,i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, label=f'Class {i} (AUC={roc_auc:.2f})')\n",
    "plt.plot([0,1],[0,1], linestyle='--', color='gray')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# CSI bar for multiclass\n",
    "if outputs.shape[1] > 1:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.bar([f'Class {i}' for i in range(num_classes)], csi, color='green')\n",
    "    plt.ylim(0,1)\n",
    "    plt.title('CSI per Class')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3783bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILE = '../Data/2025-Quantathon-Tornado-Q-training_data-640-examples.xlsx'\n",
    "TEST_FILE = '../Data/2025-Quantum-Tornado-Q-test_data-200-examples.xlsx'\n",
    "VALIDATION_FILE = '../Data/2025-Quantum-Tornado-validation_data-160-examples.xlsx'\n",
    "\n",
    "# Load training data\n",
    "df_train = pd.read_excel(TRAIN_FILE)\n",
    "# Load test data\n",
    "df_test = pd.read_excel(TEST_FILE)\n",
    "# Load validation data\n",
    "df_val = pd.read_excel(VALIDATION_FILE)\n",
    "\n",
    "# Quantum augmented datasets using random shadows\n",
    "EXTRA_TRAIN = \"../Data/36_featuresXY_train_QuantumLayers3.csv\"\n",
    "EXTRA_TEST  = \"../Data/36_featuresXY_test_QuantumLayers3.csv\"\n",
    "EXTRA_VALID = \"../Data/36_featuresXY_val_QuantumLayers3.csv\"\n",
    "\n",
    "# Extra features\n",
    "extra_train_df = pd.read_csv(EXTRA_TRAIN)\n",
    "extra_test_df = pd.read_csv(EXTRA_TEST)\n",
    "extra_valid_df = pd.read_csv(EXTRA_VALID)\n",
    "\n",
    "# Drop first column by index\n",
    "extra_train_df = extra_train_df.drop(extra_train_df.columns[0], axis=1)\n",
    "extra_test_df = extra_test_df.drop(extra_test_df.columns[0], axis=1)\n",
    "extra_valid_df = extra_valid_df.drop(extra_valid_df.columns[0], axis=1)\n",
    "\n",
    "# Concatenate extra features (axis=1 for columns)\n",
    "df_train = pd.concat([df_train, extra_train_df], axis=1)\n",
    "df_test  = pd.concat([df_test, extra_test_df], axis=1)\n",
    "df_val  = pd.concat([df_val, extra_valid_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ce45fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test, X_val, y_val = Preprocess(df_train, df_test, df_val, balance = 'smote', classes = 'binary')\n",
    "\n",
    "train_data = ClassificationDataset(X_train, y_train)\n",
    "validation_data = ClassificationDataset(X_val, y_val)\n",
    "test_data = ClassificationDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(validation_data, batch_size=64, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False, drop_last=True)\n",
    "\n",
    "X_batch, y_batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e47d9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.model_definitions import BinaryDNN_Hybrid\n",
    "DNN = BinaryDNN_Hybrid().to(device)\n",
    "\n",
    "load_path = \"checkpoints/Binary_DNN_FULLENT_AUC.6993.1011_1042.pt\"\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(load_path)\n",
    "\n",
    "# Restore model weights\n",
    "DNN.load_state_dict(checkpoint[\"DNN_state_dict\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc797a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set model to evaluation mode ---\n",
    "DNN.eval()\n",
    "\n",
    "all_targets = []\n",
    "all_preds = []\n",
    "all_outputs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for features, target in test_loader:\n",
    "        features = features.to(device)\n",
    "        target = target.to(device).float().unsqueeze(-1)\n",
    "\n",
    "        outputs = DNN(features)\n",
    "        preds = (outputs > 0.5).float()\n",
    "\n",
    "        all_targets.append(target.cpu())\n",
    "        all_preds.append(preds.cpu())\n",
    "        all_outputs.append(outputs.cpu())\n",
    "\n",
    "# --- Concatenate all batches ---\n",
    "all_targets = torch.cat(all_targets).squeeze().long().numpy()  # integers 0/1\n",
    "all_preds = torch.cat(all_preds).squeeze().long().numpy()\n",
    "all_outputs = torch.cat(all_outputs).squeeze().numpy()             # floats in [0,1]\n",
    "\n",
    "# --- Sanity check ---\n",
    "print(all_targets.shape, all_preds.shape, all_outputs.shape)\n",
    "print(np.unique(all_targets))  # should be [0,1]\n",
    "\n",
    "# --- Compute Metrics ---\n",
    "cm = confusion_matrix(all_targets, all_preds)\n",
    "auc_score = roc_auc_score(all_targets, all_outputs)  # should work now\n",
    "f1 = f1_score(all_targets, all_preds)\n",
    "acc = accuracy_score(all_targets, all_preds)\n",
    "\n",
    "# Critical Success Index (CSI)\n",
    "tp = cm[1,1]\n",
    "fn = cm[1,0]\n",
    "fp = cm[0,1]\n",
    "csi = tp / (tp + fn + fp)\n",
    "\n",
    "print(f\"AUC: {auc_score:.4f}, F1: {f1:.4f}, Accuracy: {acc:.4f}, CSI: {csi:.4f}\")\n",
    "\n",
    "# --- Plot Confusion Matrix ---\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# --- Plot ROC Curve ---\n",
    "fpr, tpr, thresholds = roc_curve(all_targets, all_outputs)\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(fpr, tpr, label=f'AUC = {auc_score:.4f}', color='blue')\n",
    "plt.plot([0,1], [0,1], linestyle='--', color='gray')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6700d5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "TRAIN_FILE = '../Data/2025-Quantathon-Tornado-Q-training_data-640-examples.xlsx'\n",
    "TEST_FILE = '../Data/2025-Quantum-Tornado-Q-test_data-200-examples.xlsx'\n",
    "VALIDATION_FILE = '../Data/2025-Quantum-Tornado-validation_data-160-examples.xlsx'\n",
    "\n",
    "# Load training data\n",
    "df_train = pd.read_excel(TRAIN_FILE)\n",
    "# Load test data\n",
    "df_test = pd.read_excel(TEST_FILE)\n",
    "# Load validation data\n",
    "df_val = pd.read_excel(VALIDATION_FILE)\n",
    "\n",
    "if True:\n",
    "    # Quantum augmented datasets using random shadows\n",
    "    EXTRA_TRAIN = \"../Data/36_featuresXY_train_QuantumLayers3.csv\"\n",
    "    EXTRA_TEST  = \"../Data/36_featuresXY_test_QuantumLayers3.csv\"\n",
    "    EXTRA_VALID = \"../Data/36_featuresXY_val_QuantumLayers3.csv\"\n",
    "    \n",
    "    # Extra features\n",
    "    extra_train_df = pd.read_csv(EXTRA_TRAIN)\n",
    "    extra_test_df = pd.read_csv(EXTRA_TEST)\n",
    "    extra_valid_df = pd.read_csv(EXTRA_VALID)\n",
    "    \n",
    "    # Drop first column by index\n",
    "    extra_train_df = extra_train_df.drop(extra_train_df.columns[0], axis=1)\n",
    "    extra_test_df = extra_test_df.drop(extra_test_df.columns[0], axis=1)\n",
    "    extra_valid_df = extra_valid_df.drop(extra_valid_df.columns[0], axis=1)\n",
    "    \n",
    "    # Concatenate extra features (axis=1 for columns)\n",
    "    df_train = pd.concat([df_train, extra_train_df], axis=1)\n",
    "    df_test  = pd.concat([df_test, extra_test_df], axis=1)\n",
    "    df_val  = pd.concat([df_val, extra_valid_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110cfe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.model_definitions import MulticlassDNN_Hybrid\n",
    "\n",
    "DNN = MulticlassDNN_Hybrid().to(device)\n",
    "\n",
    "\n",
    "load_path = \"checkpoints/Multiclass_DNN_3Layer_AUC.8.best_1011_1053.pt\"\n",
    "\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(load_path)\n",
    "\n",
    "# Restore model weights\n",
    "DNN.load_state_dict(checkpoint[\"DNN_state_dict\"])\n",
    "    \n",
    "print(f\"Loaded model from {load_path}\")\n",
    "\n",
    "\n",
    "X_train, y_train, X_test, y_test, X_val, y_val = Preprocess(df_train, df_test, df_val, balance = 'smote', classes = 'multiclass')\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "print(y_test[:])\n",
    "print(y_train[:])\n",
    "print(y_val[:])\n",
    "\n",
    "train_data = ClassificationDataset(X_train, y_train)\n",
    "validation_data = ClassificationDataset(X_val, y_val)\n",
    "test_data = ClassificationDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(validation_data, batch_size=64, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False, drop_last=True)\n",
    "\n",
    "X_batch, y_batch = next(iter(train_loader))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45030e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_targets, all_preds, all_probs = [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for features, target in test_loader:\n",
    "        features = features.to(device)\n",
    "        target = target.to(device).long()\n",
    "        outputs = DNN(features)\n",
    "        \n",
    "        if outputs.shape[1] == 1:\n",
    "            # Binary case\n",
    "            probs = torch.sigmoid(outputs).squeeze(-1)\n",
    "            preds = (probs > 0.5).long()\n",
    "        else:\n",
    "            # Multiclass case\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "        all_targets.append(target.cpu())\n",
    "        all_preds.append(preds.cpu())\n",
    "        all_probs.append(probs.cpu())\n",
    "\n",
    "# Concatenate\n",
    "all_targets = torch.cat(all_targets).numpy()\n",
    "all_preds = torch.cat(all_preds).numpy()\n",
    "all_probs = torch.cat(all_probs).numpy()\n",
    "\n",
    "# Metrics\n",
    "cm = confusion_matrix(all_targets, all_preds)\n",
    "acc = accuracy_score(all_targets, all_preds)\n",
    "if outputs.shape[1] == 1:\n",
    "    f1 = f1_score(all_targets, all_preds)\n",
    "    auc_score = roc_auc_score(all_targets, all_probs)\n",
    "    csi = cm[1,1] / (cm[1,1] + cm[1,0] + cm[0,1])\n",
    "else:\n",
    "    num_classes = outputs.shape[1]\n",
    "    f1 = f1_score(all_targets, all_preds, average='macro')\n",
    "    auc_score = roc_auc_score(np.eye(num_classes)[all_targets], all_probs, multi_class='ovr')\n",
    "    csi = []\n",
    "    for i in range(num_classes):\n",
    "        tp = cm[i,i]\n",
    "        fn = cm[i,:].sum() - tp\n",
    "        fp = cm[:,i].sum() - tp\n",
    "        csi.append(tp / (tp + fn + fp) if (tp+fn+fp)>0 else 0)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"AUC: {auc_score:.4f}, F1: {f1:.4f}, Accuracy: {acc:.4f}\")\n",
    "if outputs.shape[1] > 1:\n",
    "    for i, c in enumerate(csi):\n",
    "        print(f\"Class {i} CSI: {c:.4f}\")\n",
    "else:\n",
    "    print(f\"CSI: {csi:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# ROC curve\n",
    "plt.figure(figsize=(7,6))\n",
    "if outputs.shape[1] == 1:\n",
    "    fpr, tpr, _ = roc_curve(all_targets, all_probs)\n",
    "    plt.plot(fpr, tpr, label=f'AUC={auc_score:.4f}')\n",
    "else:\n",
    "    for i in range(num_classes):\n",
    "        fpr, tpr, _ = roc_curve((all_targets==i).astype(int), all_probs[:,i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, label=f'Class {i} (AUC={roc_auc:.2f})')\n",
    "plt.plot([0,1],[0,1], linestyle='--', color='gray')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c3d64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Data paths\n",
    "TRAIN_FILE = '../Data/2025-Quantathon-Tornado-Q-training_data-640-examples.xlsx'\n",
    "TEST_FILE = '../Data/2025-Quantum-Tornado-Q-test_data-200-examples.xlsx'\n",
    "VALIDATION_FILE = '../Data/2025-Quantum-Tornado-validation_data-160-examples.xlsx'\n",
    "\n",
    "# Load training data\n",
    "df_train = pd.read_excel(TRAIN_FILE)\n",
    "# Load test data\n",
    "df_test = pd.read_excel(TEST_FILE)\n",
    "# Load validation data\n",
    "df_val = pd.read_excel(VALIDATION_FILE)\n",
    "\n",
    "# Quantum device and parameters\n",
    "n_qubits = 8\n",
    "n_layers = 1\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "# Using Random Layers PQC\n",
    "USE_RANDOM = True\n",
    "\n",
    "if USE_RANDOM:\n",
    "    # RandomLayers expects shape (n_layers, n_rotations_per_layer)\n",
    "    # For 8 qubits, each layer typically has 3 * 8 = 24 rotations\n",
    "    rand_params = np.random.uniform(0, 2 * np.pi, (n_layers, n_qubits * 3))\n",
    "\n",
    "    @qml.qnode(dev, interface=\"torch\", diff_method=\"backprop\")\n",
    "    def quantum_feature_embedding(f, phi):\n",
    "        qml.AngleEmbedding(features=f, wires=range(n_qubits))\n",
    "        RandomLayers(phi, wires=range(n_qubits), seed=6)\n",
    "        return qml.state()  # returns 2**n_qubits = 256-dim statevector\n",
    "\n",
    "# Using Strongly Entangling Layers\n",
    "else:\n",
    "    # StronglyEntanglingLayers expects shape (n_layers, n_wires, 3)\n",
    "    rand_params = np.random.uniform(0, 2 * np.pi, (n_layers, n_qubits, 3))\n",
    "\n",
    "    @qml.qnode(dev, interface=\"torch\", diff_method=\"backprop\")\n",
    "    def quantum_feature_embedding(f, phi):\n",
    "        qml.AngleEmbedding(features=f, wires=range(n_qubits))\n",
    "        qml.StronglyEntanglingLayers(weights=phi, wires=range(n_qubits))\n",
    "        return qml.state()\n",
    "\n",
    "# Batched embedding helper\n",
    "def quantum_feature_embedding_batch(x_batch, phi, device=\"cuda\"):\n",
    "    \"\"\"Apply QNode to batch of inputs -> (B, 256)\"\"\"\n",
    "    outputs = []\n",
    "    phi = phi.detach().cpu()\n",
    "    for x in x_batch:\n",
    "        result = quantum_feature_embedding(x.detach().cpu(), phi)\n",
    "        outputs.append(result.real.to(device))\n",
    "    return torch.stack(outputs)\n",
    "\n",
    "# Test single example\n",
    "state = quantum_feature_embedding(\n",
    "    torch.tensor([1/4] * n_qubits, dtype=torch.float32),\n",
    "    torch.tensor(rand_params, dtype=torch.float32)\n",
    ")\n",
    "print(\"Single state shape:\", state.shape)  # (256,)\n",
    "\n",
    "# Test batch\n",
    "sample = torch.randn(16, n_qubits)\n",
    "phi_tensor = torch.tensor(rand_params, dtype=torch.float32)\n",
    "output = quantum_feature_embedding_batch(sample, phi_tensor, device = device)\n",
    "print(\"Batch output shape:\", output.shape)  # (16, 256)\n",
    "\n",
    "# Torch module for integration\n",
    "class QuantumFeatureEmbeddingBatch(nn.Module):\n",
    "    def __init__(self, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.phi = nn.Parameter(torch.tensor(\n",
    "            np.random.uniform(0, 2 * np.pi, (n_layers, n_qubits * 3)), \n",
    "            dtype=torch.float32\n",
    "        ))\n",
    "\n",
    "    def forward(self, x_batch):\n",
    "        outputs = []\n",
    "        phi = self.phi.detach().cpu()\n",
    "        for x in x_batch:\n",
    "            result = quantum_feature_embedding(x.detach().cpu(), phi)\n",
    "            outputs.append(result.real.to(self.device))\n",
    "        return torch.stack(outputs)\n",
    "\n",
    "# Visualize\n",
    "qml.drawer.use_style('pennylane')\n",
    "fig, ax = qml.draw_mpl(quantum_feature_embedding, level=\"device\")(f=[1/4]*n_qubits, phi=rand_params)\n",
    "fig.show()\n",
    "plt.savefig('vlad.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25619a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Binary DN with Quantum Random Layer feature encoder\n",
    "class BinaryPQC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encodes features from dataset\n",
    "        self.feature_encoder = QuantumFeatureEmbeddingBatch()\n",
    "\n",
    "        # Classifies based on encoded features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, features):\n",
    "        feats_encoded = self.feature_encoder(features)\n",
    "        class_probs = self.classifier(feats_encoded.float())\n",
    "\n",
    "        return class_probs  # Shape: (batch_size, 1)\n",
    "    \n",
    "DNN = BinaryPQC().to(device)\n",
    "    \n",
    "load_path = \"checkpoints/DNN_model_best_accuracy.pt\" # Path to load model\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(load_path, map_location=device)\n",
    "\n",
    "# Restore model weights\n",
    "DNN.load_state_dict(checkpoint[\"DNN_state_dict\"])\n",
    "    \n",
    "print(f\"Loaded model from {load_path}\")\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "X_train, y_train, X_test, y_test, X_val, y_val = Preprocess(df_train, df_test, df_val, balance = 'smote', classes = 'binary')\n",
    "\n",
    "\n",
    "train_data = ClassificationDataset(X_train, y_train)\n",
    "validation_data = ClassificationDataset(X_val, y_val)\n",
    "test_data = ClassificationDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(validation_data, batch_size=64, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False, drop_last=True)\n",
    "\n",
    "X_batch, y_batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fd265c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set model to evaluation mode ---\n",
    "DNN.eval()\n",
    "\n",
    "all_targets = []\n",
    "all_preds = []\n",
    "all_outputs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for features, target in test_loader:\n",
    "        features = features.to(device)\n",
    "        target = target.to(device).float().unsqueeze(-1)\n",
    "\n",
    "        outputs = DNN(features)\n",
    "        preds = (outputs > 0.5).float()\n",
    "\n",
    "        all_targets.append(target.cpu())\n",
    "        all_preds.append(preds.cpu())\n",
    "        all_outputs.append(outputs.cpu())\n",
    "\n",
    "# --- Concatenate all batches ---\n",
    "all_targets = torch.cat(all_targets).squeeze().long().numpy()  # integers 0/1\n",
    "all_preds = torch.cat(all_preds).squeeze().long().numpy()\n",
    "all_outputs = torch.cat(all_outputs).squeeze().numpy()             # floats in [0,1]\n",
    "\n",
    "# --- Sanity check ---\n",
    "print(all_targets.shape, all_preds.shape, all_outputs.shape)\n",
    "print(np.unique(all_targets))  # should be [0,1]\n",
    "\n",
    "# --- Compute Metrics ---\n",
    "cm = confusion_matrix(all_targets, all_preds)\n",
    "auc_score = roc_auc_score(all_targets, all_outputs)  # should work now\n",
    "f1 = f1_score(all_targets, all_preds)\n",
    "acc = accuracy_score(all_targets, all_preds)\n",
    "\n",
    "# Critical Success Index (CSI)\n",
    "tp = cm[1,1]\n",
    "fn = cm[1,0]\n",
    "fp = cm[0,1]\n",
    "csi = tp / (tp + fn + fp)\n",
    "\n",
    "print(f\"AUC: {auc_score:.4f}, F1: {f1:.4f}, Accuracy: {acc:.4f}, CSI: {csi:.4f}\")\n",
    "\n",
    "# --- Plot Confusion Matrix ---\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ca177a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "TRAIN_FILE = '../Data/2025-Quantathon-Tornado-Q-training_data-640-examples.xlsx'\n",
    "TEST_FILE = '../Data/2025-Quantum-Tornado-Q-test_data-200-examples.xlsx'\n",
    "VALIDATION_FILE = '../Data/2025-Quantum-Tornado-validation_data-160-examples.xlsx'\n",
    "\n",
    "# Load training data\n",
    "df_train = pd.read_excel(TRAIN_FILE)\n",
    "# Load test data\n",
    "df_test = pd.read_excel(TEST_FILE)\n",
    "# Load validation data\n",
    "df_val = pd.read_excel(VALIDATION_FILE)\n",
    "\n",
    "# Quantum device and parameters\n",
    "n_qubits = 8\n",
    "n_layers = 1\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "# Using Random Layers PQC\n",
    "USE_RANDOM = True\n",
    "\n",
    "if USE_RANDOM:\n",
    "    # RandomLayers expects shape (n_layers, n_rotations_per_layer)\n",
    "    # For 8 qubits, each layer typically has 3 * 8 = 24 rotations\n",
    "    rand_params = np.random.uniform(0, 2 * np.pi, (n_layers, n_qubits * 3))\n",
    "    n_qubits = 8\n",
    "    n_layers = 1\n",
    "    phi = nn.Parameter(torch.randn(n_layers, n_qubits, 9))\n",
    "\n",
    "    @qml.qnode(dev, interface=\"torch\", diff_method=\"backprop\")\n",
    "    def quantum_feature_embedding(f, phi):\n",
    "        qml.AngleEmbedding(features=f, wires=range(n_qubits))\n",
    "        RandomLayers(phi, wires=range(n_qubits), seed=6)\n",
    "        return qml.state()  # returns 2**n_qubits = 256-dim statevector\n",
    "\n",
    "# Using Strongly Entangling Layers\n",
    "else:\n",
    "    # StronglyEntanglingLayers expects shape (n_layers, n_wires, 3)\n",
    "    rand_params = np.random.uniform(0, 2 * np.pi, (n_layers, n_qubits, 3))\n",
    "\n",
    "    @qml.qnode(dev, interface=\"torch\", diff_method=\"backprop\")\n",
    "    def quantum_feature_embedding(f, phi):\n",
    "        qml.AngleEmbedding(features=f, wires=range(n_qubits))\n",
    "        qml.StronglyEntanglingLayers(weights=phi, wires=range(n_qubits))\n",
    "        return qml.state()\n",
    "\n",
    "# Batched embedding helper\n",
    "def quantum_feature_embedding_batch(x_batch, phi, device=\"cuda\"):\n",
    "    \"\"\"Apply QNode to batch of inputs -> (B, 256)\"\"\"\n",
    "    outputs = []\n",
    "    phi = phi.detach().cpu()\n",
    "    for x in x_batch:\n",
    "        result = quantum_feature_embedding(x.detach().cpu(), phi)\n",
    "        outputs.append(result.real.to(device))\n",
    "    return torch.stack(outputs)\n",
    "\n",
    "# Test single example\n",
    "state = quantum_feature_embedding(\n",
    "    torch.tensor([1/4] * n_qubits, dtype=torch.float32),\n",
    "    torch.tensor(rand_params, dtype=torch.float32)\n",
    ")\n",
    "print(\"Single state shape:\", state.shape)  # (256,)\n",
    "\n",
    "# Test batch\n",
    "sample = torch.randn(16, n_qubits)\n",
    "phi_tensor = torch.tensor(rand_params, dtype=torch.float32)\n",
    "output = quantum_feature_embedding_batch(sample, phi_tensor, device = device)\n",
    "print(\"Batch output shape:\", output.shape)  # (16, 256)\n",
    "\n",
    "# Torch module for integration\n",
    "class QuantumFeatureEmbeddingBatch(nn.Module):\n",
    "    def __init__(self, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.phi = nn.Parameter(torch.tensor(\n",
    "            np.random.uniform(0, 2 * np.pi, (n_layers, n_qubits * 3)), \n",
    "            dtype=torch.float32\n",
    "        ))\n",
    "\n",
    "    def forward(self, x_batch):\n",
    "        outputs = []\n",
    "        phi = self.phi.detach().cpu()\n",
    "        for x in x_batch:\n",
    "            result = quantum_feature_embedding(x.detach().cpu(), phi)\n",
    "            outputs.append(result.real.to(self.device))\n",
    "        return torch.stack(outputs)\n",
    "\n",
    "# Visualize\n",
    "qml.drawer.use_style('pennylane')\n",
    "fig, ax = qml.draw_mpl(quantum_feature_embedding, level=\"device\")(f=[1/4]*n_qubits, phi=rand_params)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d964abe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiclass QNN with Quantum Random Layer feature encoder\n",
    "class MulticlassQNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encodes features from dataset\n",
    "        self.feature_encoder = QuantumFeatureEmbeddingBatch()\n",
    "\n",
    "        # Classifies based on encoded features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 4)\n",
    "        )\n",
    "\n",
    "    def forward(self, features):\n",
    "        feats_encoded = self.feature_encoder(features)\n",
    "        class_probs = self.classifier(feats_encoded.float())\n",
    "\n",
    "        return class_probs  # Shape: (batch_size, 1)\n",
    "    \n",
    "X_train, y_train, X_test, y_test, X_val, y_val = Preprocess(df_train, df_test, df_val, balance = 'smote', classes = 'multiclass')\n",
    "\n",
    "print(y_test[:])\n",
    "print(y_train[:])\n",
    "print(y_val[:])\n",
    "\n",
    "train_data = ClassificationDataset(X_train, y_train)\n",
    "validation_data = ClassificationDataset(X_val, y_val)\n",
    "test_data = ClassificationDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(validation_data, batch_size=64, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False, drop_last=True)\n",
    "\n",
    "X_batch, y_batch = next(iter(train_loader))\n",
    "\n",
    "DNN = MulticlassQNN().to(device)\n",
    "\n",
    "load_path = \"checkpoints/DNN_model_best_accuracy_RLMulti.pt\" # Path to load model\n",
    "\n",
    "checkpoint = torch.load(load_path, map_location= device)\n",
    "\n",
    "# Restore model weights\n",
    "DNN.load_state_dict(checkpoint[\"DNN_state_dict\"])\n",
    "    \n",
    "print(f\"Loaded model from {load_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f27e57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DNN.eval()\n",
    "all_targets, all_preds, all_probs = [], [], []\n",
    "\n",
    "with torch.no_grad():>\n",
    "    for features, target in test_loader:\n",
    "        features = features.to(device)\n",
    "        target = target.to(device).long()\n",
    "        outputs = DNN(features)\n",
    "        \n",
    "        if outputs.shape[1] == 1:\n",
    "            # Binary case\n",
    "            probs = torch.sigmoid(outputs).squeeze(-1)\n",
    "            preds = (probs > 0.5).long()\n",
    "        else:\n",
    "            # Multiclass case\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    ">\n",
    "        all_targets.append(target.cpu())\n",
    "        all_preds.append(preds.cpu())\n",
    "        all_probs.append(probs.cpu())\n",
    "\n",
    "# Concatenate\n",
    "all_targets = torch.cat(all_targets).numpy()\n",
    "all_preds = torch.cat(all_preds).numpy()\n",
    "all_probs = torch.cat(all_probs).numpy()\n",
    "\n",
    "# Metrics\n",
    "cm = confusion_matrix(all_targets, all_preds)\n",
    "acc = accuracy_score(all_targets, all_preds)\n",
    "if outputs.shape[1] == 1:\n",
    "    f1 = f1_score(all_targets, all_preds)\n",
    "    auc_score = roc_auc_score(all_targets, all_probs)\n",
    "    csi = cm[1,1] / (cm[1,1] + cm[1,0] + cm[0,1])\n",
    "else:\n",
    "    num_classes = outputs.shape[1]\n",
    "    f1 = f1_score(all_targets, all_preds, average='macro')\n",
    "    auc_score = roc_auc_score(np.eye(num_classes)[all_targets], all_probs, multi_class='ovr')\n",
    "    csi = []\n",
    "    for i in range(num_classes):\n",
    "        tp = cm[i,i]\n",
    "        fn = cm[i,:].sum() - tp\n",
    "        fp = cm[:,i].sum() - tp\n",
    "        csi.append(tp / (tp + fn + fp) if (tp+fn+fp)>0 else 0)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"AUC: {auc_score:.4f}, F1: {f1:.4f}, Accuracy: {acc:.4f}\")\n",
    "if outputs.shape[1] > 1:\n",
    "    for i, c in enumerate(csi):\n",
    "        print(f\"Class {i} CSI: {c:.4f}\")\n",
    "else:\n",
    "    print(f\"CSI: {csi:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# ROC curve\n",
    "plt.figure(figsize=(7,6))\n",
    "if outputs.shape[1] == 1:\n",
    "    fpr, tpr, _ = roc_curve(all_targets, all_probs)\n",
    "    plt.plot(fpr, tpr, label=f'AUC={auc_score:.4f}')\n",
    "else:\n",
    "    for i in range(num_classes):\n",
    "        fpr, tpr, _ = roc_curve((all_targets==i).astype(int), all_probs[:,i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, label=f'Class {i} (AUC={roc_auc:.2f})')\n",
    "plt.plot([0,1],[0,1], linestyle='--', color='gray')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b019fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "circuits[0].draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6449fd44",
   "metadata": {},
   "source": [
    "print(cfg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 [Default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
